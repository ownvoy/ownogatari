<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Example | 미래사회와 창의혁신인재</title>
<meta name="keywords" content="">
<meta name="description" content="Self notes Research
  Methodology::
  Key Constructs::
   - IVs::
 - DVs::
 - Moderators::
 - Others::
  Key_Findings::
  Contributions::
  Limitations::
  Self Critique
 Critique   - Pros
 - Cons
 How is it relevant to my research?   - Relevant_topic::
 - Use::
 [!Cite]   Mildenhall, Ben, Pratul P. Srinivasan, Matthew Tancik, Jonathan T.">
<meta name="author" content="">
<link rel="canonical" href="https://ownogatari.xyz/posts/example/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.735c14aef5bd53538764fbe842da3b6b2041059e13045d88f457bc438e58e012.css" integrity="sha256-c1wUrvW9U1OHZPvoQto7ayBBBZ4TBF2I9Fe8Q45Y4BI=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://ownogatari.xyz/images/favicon/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://ownogatari.xyz/images/favicon/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://ownogatari.xyz/images/favicon/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://ownogatari.xyz/images/favicon/apple-touch-icon.png">
<link rel="mask-icon" href="https://ownogatari.xyz/images/favicon/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Example" />
<meta property="og:description" content="Self notes Research
  Methodology::
  Key Constructs::
   - IVs::
 - DVs::
 - Moderators::
 - Others::
  Key_Findings::
  Contributions::
  Limitations::
  Self Critique
 Critique   - Pros
 - Cons
 How is it relevant to my research?   - Relevant_topic::
 - Use::
 [!Cite]   Mildenhall, Ben, Pratul P. Srinivasan, Matthew Tancik, Jonathan T." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ownogatari.xyz/posts/example/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-27T21:56:50+09:00" />
<meta property="article:modified_time" content="2023-10-27T21:56:50+09:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Example"/>
<meta name="twitter:description" content="Self notes Research
  Methodology::
  Key Constructs::
   - IVs::
 - DVs::
 - Moderators::
 - Others::
  Key_Findings::
  Contributions::
  Limitations::
  Self Critique
 Critique   - Pros
 - Cons
 How is it relevant to my research?   - Relevant_topic::
 - Use::
 [!Cite]   Mildenhall, Ben, Pratul P. Srinivasan, Matthew Tancik, Jonathan T."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Example",
      "item": "https://ownogatari.xyz/posts/example/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Example",
  "name": "Example",
  "description": "Self notes Research\n  Methodology::\n  Key Constructs::\n   - IVs::\n - DVs::\n - Moderators::\n - Others::\n  Key_Findings::\n  Contributions::\n  Limitations::\n  Self Critique\n Critique   - Pros\n - Cons\n How is it relevant to my research?   - Relevant_topic::\n - Use::\n [!Cite]   Mildenhall, Ben, Pratul P. Srinivasan, Matthew Tancik, Jonathan T.",
  "keywords": [
    
  ],
  "articleBody": "Self notes Research\n  Methodology::\n  Key Constructs::\n   - IVs::\n - DVs::\n - Moderators::\n - Others::\n  Key_Findings::\n  Contributions::\n  Limitations::\n  Self Critique\n Critique   - Pros\n - Cons\n How is it relevant to my research?   - Relevant_topic::\n - Use::\n [!Cite]   Mildenhall, Ben, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. “NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis.” arXiv, August 3, 2020. https://doi.org/10.48550/arXiv.2003.08934.\n  [!Synth]   Contribution::\n  [!md]   Author:: Mildenhall, Ben  Author:: Srinivasan, Pratul P.  Author:: Tancik, Matthew  Author:: Barron, Jonathan T.  Author:: Ramamoorthi, Ravi  Author:: Ng, Ren     Title:: NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis   Year:: 2020   Citekey:: @MildenhallEtAl2020    Tags:: Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics    itemType:: preprint                        DOI:: 10.48550/arXiv.2003.08934     [!LINK]      Mildenhall et al_2020_NeRF.pdf.\n  [!Abstract]      abstract:: We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.       Annotations  [!Highlight]\n represent this function by regressing from a single 5D coordinate (x, y, z, θ, φ) to a single volume density and view-dependent RGB color.(1)\n   [!Highlight]\n basic implementation of optimizing a neural radiance field representation for a complex scene does not converge to a sufficiently highresolution representation and is inefficient in the required number of samples per camera ray.(2)\n   [!Highlight]\n We address these issues by transforming input 5D coordinates with a positional encoding that enables the MLP to represent higher frequency functions, and we propose a hierarchical sampling procedure to reduce the number of queries required to adequately sample this high-frequency scene representation.(2)\n   [!Highlight]\n  our method overcomes the prohibitive storage costs of discretized voxel grids when modeling complex scenes at high-resolutions.(2)\n    [!Highlight]\n  directly maps from a 3D spatial location to an implicit representation of the shape(3)\n    [!Highlight]\n  discrete representations(3)\n    [!Highlight]\n  implicit representation of continuous 3D shapes as level sets by optimizing deep networks that map xyz coordinates to signed distance functions [15,32] or occupancy fields [11,27](3)\n    Comment:\n    *Signed distance functions (SDFs) and occupancy fields (OFs) are two different ways of representing 3D shapes implicitly.\n Signed distance functionsmeasure the distance from a point to the nearest surface of a shape. They are typically represented as a volumetric field, where the value at each point is the signed distance to the nearest surface. A negative distance indicates that the point is inside the shape, while a positive distance indicates that the point is outside the shape.\nOccupancy fields represent the probability that a given point is inside a shape. They are also typically represented as a volumetric field, where the value at each point is the probability that the point is inside the shape. A probability of 1 indicates that the point is definitely inside the shape, while a probability of 0 indicates that the point is definitely outside the shape.\nBoth SDFs and OFs have their own advantages and disadvantages. SDFs are more accurate, but they can be more difficult to compute and optimize. OFs are easier to compute and optimize, but they can be less accurate.*\n [!Highlight]\n  these models are limited by their requirement of access to ground truth 3D geometry,(3)\n    [!Highlight]\n  Niemeyer et al. [29] represent surfaces as 3D occupancy fields and use a numerical method to find the surface intersection for each ray, then calculate an exact derivative using implicit differentiation. Each ray intersection location is provided as the input to a neural 3D texture field that predicts a diffuse color for that point.(3)\n    Comment:\n    *In the paper by Niemeyer et al., the authors propose a method for representing surfaces as 3D occupancy fields. An occupancy field is a function that maps each point in space to a value between 0 and 1, where 1 indicates that the point is occupied by the surface and 0 indicates that the point is not occupied by the surface. The authors use a neural network to represent the occupancy field. The input to the neural network is a point in space and the output is the probability that the point is occupied by the surface.To find the surface intersection for each ray, the authors use a numerical method. This method casts a ray through the scene and then searches for the point where the ray intersects the surface. Once the surface intersection has been found, the authors calculate the exact derivative using implicit differentiation. Implicit\n differentiation is a technique for calculating the derivative of a function without explicitly defining the function.The authors then use the surface intersection location as the input to a neural 3D texture field. The 3D texture field is a neural network that predicts a diffuse color for a given point in space. The diffuse color is the color of the surface at a given point, assuming that the surface is lit by a uniform light source.*\n [!Highlight]\n  represent higher-resolution geometry and appearance to render photorealistic novel views of complex scenes.(4)\n    Comment:\n    우리 모델의 장점1\n  [!Highlight]\n  One popular class of approaches uses mesh-based representations of scenes with either diffuse [48] or view-dependent [2,8,49] appearance.(4)\n    Comment:\n    *A mesh-based representation is a way to represent a 3D object or scene as a collection of interconnected polygons. Each polygon has a set of vertices, which are points in 3D space. The vertices are connected by edges, and the edges define the faces of the polygon.\n There are two main types of mesh-based representations:\nDiffuse mesh-based representations:These representations encode the appearance of a scene using a single color value per mesh vertex. This is a simple and efficient representation, but it can only capture diffuse appearance effects.\nView-dependent mesh-based representations:These representations encode the appearance of a scene using a different color value for each mesh vertex, depending on the viewing direction. This allows for the representation of more complex appearance effects, such as specular highlights and reflections.*\n [!Highlight]\n  Differentiable rasterizers [4,10,23,25] or pathtracers [22,30] can directly optimize mesh representations to reproduce a set of input images using gradient descent.(4)\n    [!Highlight]\n  this strategy requires a template mesh with fixed topology(4)\n    [!Highlight]\n  Volumetric approaches are able to realistically represent complex shapes and materials, are well-suited for gradient-based optimization, and tend to produce less visually distracting artifacts than mesh-based methods.(4)\n    [!Highlight]\n  alpha-compositing(4)\n    [!Highlight]\n  While these volumetric techniques have achieved impressive results for novel view synthesis, their ability to scale to higher resolution imagery is fundamentally limited by poor time and space complexity due to their discrete sampling(4)\n    [!Highlight]\n  We circumvent this problem by instead encoding a continuous volume within the parameters of a deep fully-connected neural network,(4)\n    [!Highlight]\n  5D vector-valued function whose input is a 3D location x = (x, y, z) and 2D viewing direction (θ, φ), and whose output is an emitted color c = (r, g, b) and volume density σ.(4)\n    [!Image]\n  Image  ![[Archive/zotero/MildenhallEtAl2020/MildenhallEtAl2020-5-x26-y472.png]]\n    [!Highlight]\n  direction as a 3D Cartesian unit vector d(5)\n    [!Highlight]\n  MLP network FΘ : (x, d) → (c, σ)(5)\n    [!Highlight]\n  the volume density σ as a function of only the location x, while allowing the RGB color c to be predicted as a function of both location and viewing direction.(5)\n    Comment:\n    density는 위치(x)만을 고려하고, color는 위치(x), 보는 각도(viewing direction) 둘 다 고려함\n  [!Highlight]\n  volume density σ(x)(5)\n    [!Highlight]\n  The expected color C(r)(5)\n    [!Highlight]\n  ray r(t) = o + td(5)\n    [!Image]\n  Image  ![[Archive/zotero/MildenhallEtAl2020/MildenhallEtAl2020-5-x34-y24.png]]\n    [!Highlight]\n  T (t) denotes the accumulated transmittance along the ray from tn to t, i.e., the probability that the ray travels from tn to t without hitting any other particle.(6)\n    [!Highlight]\n  we use a stratified sampling approach where we partition [tn, tf ] into N evenly-spaced bins and then draw one sample uniformly at random from within each bin:(6)\n    [!Image]\n  Image  ![[Archive/zotero/MildenhallEtAl2020/MildenhallEtAl2020-6-x34-y182.png]]\n    [!Highlight]\n  stratified sampling enables us to represent a continuous scene representation because it results in the MLP being evaluated at continuous positions over the course of optimization.(6)\n    [!Image]\n  Image  ![[Archive/zotero/MildenhallEtAl2020/MildenhallEtAl2020-6-x35-y70.png]]\n    [!Image]\n  Image  ![[Archive/zotero/MildenhallEtAl2020/MildenhallEtAl2020-9-x158-y91.png]]\n    ",
  "wordCount" : "1441",
  "inLanguage": "en",
  "datePublished": "2023-10-27T21:56:50+09:00",
  "dateModified": "2023-10-27T21:56:50+09:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://ownogatari.xyz/posts/example/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "미래사회와 창의혁신인재",
    "logo": {
      "@type": "ImageObject",
      "url": "https://ownogatari.xyz/images/favicon/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://ownogatari.xyz/" accesskey="h" title="미래사회와 창의혁신인재 (Alt + H)">미래사회와 창의혁신인재</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://ownogatari.xyz/categories/" title="categories">
                    <span>categories</span>
                </a>
            </li>
            <li>
                <a href="https://ownogatari.xyz/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://ownogatari.xyz/">Home</a></div>
    <h1 class="post-title">
      Example
    </h1>
    <div class="post-meta"><span title='2023-10-27 21:56:50 +0900 KST'>October 27, 2023</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;1441 words

</div>
  </header> 
  <div class="post-content"><!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h2 id="self-notes">Self notes<a hidden class="anchor" aria-hidden="true" href="#self-notes">#</a></h2>
<p><strong>Research</strong></p>
<ul>
<li>
<p>Methodology::</p>
</li>
<li>
<p>Key Constructs::</p>
</li>
</ul>
<p>    - IVs::</p>
<p>    - DVs::</p>
<p>    - Moderators::</p>
<p>    - Others::</p>
<ul>
<li>
<p>Key_Findings::</p>
</li>
<li>
<p>Contributions::</p>
</li>
<li>
<p>Limitations::</p>
</li>
</ul>
<p><strong>Self Critique</strong></p>
<ul>
<li>Critique</li>
</ul>
<p>    - Pros</p>
<p>    - Cons</p>
<ul>
<li>How is it relevant to my research?</li>
</ul>
<p>    - Relevant_topic::</p>
<p>    - Use::</p>
<blockquote>
<p>[!Cite]  </p>
</blockquote>
<blockquote>
<p>Mildenhall, Ben, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. “NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis.” arXiv, August 3, 2020. <a href="https://doi.org/10.48550/arXiv.2003.08934">https://doi.org/10.48550/arXiv.2003.08934</a>.</p>
</blockquote>
<blockquote>
<p>[!Synth]  </p>
</blockquote>
<blockquote>
<p><strong>Contribution</strong>::</p>
</blockquote>
<blockquote>
<p>[!md]  </p>
</blockquote>
<blockquote>
<p><strong>Author</strong>:: Mildenhall, Ben<!-- raw HTML omitted -->  </p>
</blockquote>
<blockquote>
<p><strong>Author</strong>:: Srinivasan, Pratul P.<!-- raw HTML omitted -->  </p>
</blockquote>
<blockquote>
<p><strong>Author</strong>:: Tancik, Matthew<!-- raw HTML omitted -->  </p>
</blockquote>
<blockquote>
<p><strong>Author</strong>:: Barron, Jonathan T.<!-- raw HTML omitted -->  </p>
</blockquote>
<blockquote>
<p><strong>Author</strong>:: Ramamoorthi, Ravi<!-- raw HTML omitted -->  </p>
</blockquote>
<blockquote>
<p><strong>Author</strong>:: Ng, Ren<!-- raw HTML omitted -->  </p>
</blockquote>
<blockquote>
<p> </p>
</blockquote>
<blockquote>
<p><strong>Title</strong>:: NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis  </p>
</blockquote>
<blockquote>
<p><strong>Year</strong>:: 2020  </p>
</blockquote>
<blockquote>
<p><strong>Citekey</strong>:: @MildenhallEtAl2020  </p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>Tags</strong>:: Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics  </p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>itemType</strong>:: preprint  </p>
</blockquote>
<blockquote>
<p> </p>
</blockquote>
<blockquote>
<p> </p>
</blockquote>
<blockquote>
<p> </p>
</blockquote>
<blockquote>
<p> </p>
</blockquote>
<blockquote>
<p> </p>
</blockquote>
<blockquote>
<p> </p>
</blockquote>
<blockquote>
<p> </p>
</blockquote>
<blockquote>
<p><strong>DOI</strong>:: 10.48550/arXiv.2003.08934  </p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>[!LINK]  </p>
</blockquote>
<blockquote>
<p> </p>
</blockquote>
<blockquote>
<p><a href="">Mildenhall et al_2020_NeRF.pdf</a>.</p>
</blockquote>
<blockquote>
<p>[!Abstract]  </p>
</blockquote>
<blockquote>
<p> </p>
</blockquote>
<blockquote>
<p>abstract:: We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\theta, \phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.  </p>
</blockquote>
<blockquote>
<blockquote>
</blockquote>
</blockquote>
<hr>
<h2 id="annotations">Annotations<a hidden class="anchor" aria-hidden="true" href="#annotations">#</a></h2>
<blockquote>
<p>[!Highlight]</p>
</blockquote>
<p><!-- raw HTML omitted -->represent this function by regressing from a single 5D coordinate (x, y, z, θ, φ) to a single volume density and view-dependent RGB color.<!-- raw HTML omitted --> (<a href="zotero://open-pdf/library/items/NEZW66PK?page=1&amp;annotation=C62I6AC9">1</a>)</p>
<blockquote>
</blockquote>
<blockquote>
<p>[!Highlight]</p>
</blockquote>
<p><!-- raw HTML omitted -->basic implementation of optimizing a neural radiance field representation for a complex scene does not converge to a sufficiently highresolution representation and is inefficient in the required number of samples per camera ray.<!-- raw HTML omitted --> (<a href="zotero://open-pdf/library/items/NEZW66PK?page=2&amp;annotation=4MC36DXS">2</a>)</p>
<blockquote>
</blockquote>
<blockquote>
<p>[!Highlight]</p>
</blockquote>
<p><!-- raw HTML omitted -->We address these issues by transforming input 5D coordinates with a positional encoding that enables the MLP to represent higher frequency functions, and we propose a hierarchical sampling procedure to reduce the number of queries required to adequately sample this high-frequency scene representation.<!-- raw HTML omitted --> (<a href="zotero://open-pdf/library/items/NEZW66PK?page=2&amp;annotation=VAVW7VPG">2</a>)</p>
<blockquote>
</blockquote>
<blockquote>
<p>[!Highlight]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted -->our method overcomes the prohibitive storage costs of discretized voxel grids when modeling complex scenes at high-resolutions.<!-- raw HTML omitted --> (<a href="zotero://open-pdf/library/items/NEZW66PK?page=2&amp;annotation=B5A895WQ">2</a>)</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>[!Highlight]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted -->directly maps from a 3D spatial location to an implicit representation of the shape<!-- raw HTML omitted --> (<a href="zotero://open-pdf/library/items/NEZW66PK?page=3&amp;annotation=RRCN87AY">3</a>)</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>[!Highlight]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted -->discrete representations<!-- raw HTML omitted --> (<a href="zotero://open-pdf/library/items/NEZW66PK?page=3&amp;annotation=3NXWTU8N">3</a>)</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>[!Highlight]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted -->implicit representation of continuous 3D shapes as level sets by optimizing deep networks that map xyz coordinates to signed distance functions [15,32] or occupancy fields [11,27]<!-- raw HTML omitted --> (<a href="zotero://open-pdf/library/items/NEZW66PK?page=3&amp;annotation=FVAQMR7Z">3</a>)</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p><strong>Comment</strong>:</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>*Signed distance functions (SDFs) and occupancy fields (OFs) are two different ways of representing 3D shapes implicitly.</p>
</blockquote>
<p><!-- raw HTML omitted -->Signed distance functions<!-- raw HTML omitted --> measure the distance from a point to the nearest surface of a shape. They are typically represented as a volumetric field, where the value at each point is the signed distance to the nearest surface. A negative distance indicates that the point is inside the shape, while a positive distance indicates that the point is outside the shape.</p>
<p>Occupancy fields represent the probability that a given point is inside a shape. They are also typically represented as a volumetric field, where the value at each point is the probability that the point is inside the shape. A probability of 1 indicates that the point is definitely inside the shape, while a probability of 0 indicates that the point is definitely outside the shape.</p>
<p>Both SDFs and OFs have their own advantages and disadvantages. SDFs are more accurate, but they can be more difficult to compute and optimize. OFs are easier to compute and optimize, but they can be less accurate.*</p>
<blockquote>
<p>[!Highlight]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted -->these models are limited by their requirement of access to ground truth 3D geometry,<!-- raw HTML omitted --> (<a href="zotero://open-pdf/library/items/NEZW66PK?page=3&amp;annotation=3K7AZQXG">3</a>)</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>[!Highlight]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted -->Niemeyer et al. [29] represent surfaces as 3D occupancy fields and use a numerical method to find the surface intersection for each ray, then calculate an exact derivative using implicit differentiation. Each ray intersection location is provided as the input to a neural 3D texture field that predicts a diffuse color for that point.<!-- raw HTML omitted --> (<a href="zotero://open-pdf/library/items/NEZW66PK?page=3&amp;annotation=IBYZPKD4">3</a>)</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p><strong>Comment</strong>:</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>*In the paper by Niemeyer et al., the authors propose a method for representing surfaces as 3D occupancy fields. An occupancy field is a function that maps each point in space to a value between 0 and 1, where 1 indicates that the point is occupied by the surface and 0 indicates that the point is not occupied by the surface. The authors use a neural network to represent the occupancy field. The input to the neural network is a point in space and the output is the probability that the point is occupied by the surface.To find the surface intersection for each ray, the authors use a numerical method. This method casts a ray through the scene and then searches for the point where the ray intersects the surface. Once the surface intersection has been found, the authors calculate the exact derivative using implicit differentiation. Implicit</p>
</blockquote>
<p>differentiation is a technique for calculating the derivative of a function without explicitly defining the function.The authors then use the surface intersection location as the input to a neural 3D texture field. The 3D texture field is a neural network that predicts a diffuse color for a given point in space. The diffuse color is the color of the surface at a given point, assuming that the surface is lit by a uniform light source.*</p>
<blockquote>
<p>[!Highlight]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted -->represent higher-resolution geometry and appearance to render photorealistic novel views of complex scenes.<!-- raw HTML omitted --> (<a href="zotero://open-pdf/library/items/NEZW66PK?page=4&amp;annotation=NCNPXP7J">4</a>)</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p><strong>Comment</strong>:</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p><em>우리 모델의 장점1</em></p>
</blockquote>
<blockquote>
<p>[!Highlight]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted -->One popular class of approaches uses mesh-based representations of scenes with either diffuse [48] or view-dependent [2,8,49] appearance.<!-- raw HTML omitted --> (<a href="zotero://open-pdf/library/items/NEZW66PK?page=4&amp;annotation=M4PCFGFU">4</a>)</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p><strong>Comment</strong>:</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>*A mesh-based representation is a way to represent a 3D object or scene as a collection of interconnected polygons. Each polygon has a set of vertices, which are points in 3D space. The vertices are connected by edges, and the edges define the faces of the polygon.</p>
</blockquote>
<p>There are two main types of mesh-based representations:</p>
<p><!-- raw HTML omitted -->Diffuse mesh-based representations:<!-- raw HTML omitted --> These representations encode the appearance of a scene using a single color value per mesh vertex. This is a simple and efficient representation, but it can only capture diffuse appearance effects.</p>
<p><!-- raw HTML omitted -->View-dependent mesh-based representations:<!-- raw HTML omitted --> These representations encode the appearance of a scene using a different color value for each mesh vertex, depending on the viewing direction. This allows for the representation of more complex appearance effects, such as specular highlights and reflections.*</p>
<blockquote>
<p>[!Highlight]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted -->Differentiable rasterizers [4,10,23,25] or pathtracers [22,30] can directly optimize mesh representations to reproduce a set of input images using gradient descent.<!-- raw HTML omitted --> (<a href="zotero://open-pdf/library/items/NEZW66PK?page=4&amp;annotation=9JD2H4II">4</a>)</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>[!Highlight]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted -->this strategy requires a template mesh with fixed topology<!-- raw HTML omitted --> (<a href="zotero://open-pdf/library/items/NEZW66PK?page=4&amp;annotation=UF3TZXTF">4</a>)</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>[!Highlight]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted -->Volumetric approaches are able to realistically represent complex shapes and materials, are well-suited for gradient-based optimization, and tend to produce less visually distracting artifacts than mesh-based methods.<!-- raw HTML omitted --> (<a href="zotero://open-pdf/library/items/NEZW66PK?page=4&amp;annotation=MQG2F6FD">4</a>)</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>[!Highlight]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted -->alpha-compositing<!-- raw HTML omitted --> (<a href="zotero://open-pdf/library/items/NEZW66PK?page=4&amp;annotation=IGK66V5M">4</a>)</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>[!Highlight]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted -->While these volumetric techniques have achieved impressive results for novel view synthesis, their ability to scale to higher resolution imagery is fundamentally limited by poor time and space complexity due to their discrete sampling<!-- raw HTML omitted --> (<a href="zotero://open-pdf/library/items/NEZW66PK?page=4&amp;annotation=N93G7Z5N">4</a>)</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>[!Highlight]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted -->We circumvent this problem by instead encoding a continuous volume within the parameters of a deep fully-connected neural network,<!-- raw HTML omitted --> (<a href="zotero://open-pdf/library/items/NEZW66PK?page=4&amp;annotation=A9M2CZBR">4</a>)</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>[!Highlight]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted -->5D vector-valued function whose input is a 3D location x = (x, y, z) and 2D viewing direction (θ, φ), and whose output is an emitted color c = (r, g, b) and volume density σ.<!-- raw HTML omitted --> (<a href="zotero://open-pdf/library/items/NEZW66PK?page=4&amp;annotation=28GBIAWI">4</a>)</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>[!Image]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted --> Image<!-- raw HTML omitted --></p>
</blockquote>
<blockquote>
<p>![[Archive/zotero/MildenhallEtAl2020/MildenhallEtAl2020-5-x26-y472.png]]</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>[!Highlight]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted -->direction as a 3D Cartesian unit vector d<!-- raw HTML omitted --> (<a href="zotero://open-pdf/library/items/NEZW66PK?page=5&amp;annotation=PVDGTPPU">5</a>)</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>[!Highlight]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted -->MLP network FΘ : (x, d) → (c, σ)<!-- raw HTML omitted --> (<a href="zotero://open-pdf/library/items/NEZW66PK?page=5&amp;annotation=4QG9N9DY">5</a>)</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>[!Highlight]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted -->the volume density σ as a function of only the location x, while allowing the RGB color c to be predicted as a function of both location and viewing direction.<!-- raw HTML omitted --> (<a href="zotero://open-pdf/library/items/NEZW66PK?page=5&amp;annotation=XQPF8JSX">5</a>)</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p><strong>Comment</strong>:</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p><em>density는 위치(x)만을 고려하고, color는 위치(x), 보는 각도(viewing direction) 둘 다 고려함</em></p>
</blockquote>
<blockquote>
<p>[!Highlight]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted -->volume density σ(x)<!-- raw HTML omitted --> (<a href="zotero://open-pdf/library/items/NEZW66PK?page=5&amp;annotation=PZDVA5CS">5</a>)</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>[!Highlight]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted -->The expected color C(r)<!-- raw HTML omitted --> (<a href="zotero://open-pdf/library/items/NEZW66PK?page=5&amp;annotation=DDQWGWT8">5</a>)</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>[!Highlight]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted -->ray r(t) = o + td<!-- raw HTML omitted --> (<a href="zotero://open-pdf/library/items/NEZW66PK?page=5&amp;annotation=K3U38CZE">5</a>)</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>[!Image]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted --> Image<!-- raw HTML omitted --></p>
</blockquote>
<blockquote>
<p>![[Archive/zotero/MildenhallEtAl2020/MildenhallEtAl2020-5-x34-y24.png]]</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>[!Highlight]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted -->T (t) denotes the accumulated transmittance along the ray from tn to t, i.e., the probability that the ray travels from tn to t without hitting any other particle.<!-- raw HTML omitted --> (<a href="zotero://open-pdf/library/items/NEZW66PK?page=6&amp;annotation=B3C6L49Q">6</a>)</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>[!Highlight]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted -->we use a stratified sampling approach where we partition [tn, tf ] into N evenly-spaced bins and then draw one sample uniformly at random from within each bin:<!-- raw HTML omitted --> (<a href="zotero://open-pdf/library/items/NEZW66PK?page=6&amp;annotation=BP6LRDD5">6</a>)</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>[!Image]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted --> Image<!-- raw HTML omitted --></p>
</blockquote>
<blockquote>
<p>![[Archive/zotero/MildenhallEtAl2020/MildenhallEtAl2020-6-x34-y182.png]]</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>[!Highlight]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted -->stratified sampling enables us to represent a continuous scene representation because it results in the MLP being evaluated at continuous positions over the course of optimization.<!-- raw HTML omitted --> (<a href="zotero://open-pdf/library/items/NEZW66PK?page=6&amp;annotation=JBF2KHAN">6</a>)</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>[!Image]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted --> Image<!-- raw HTML omitted --></p>
</blockquote>
<blockquote>
<p>![[Archive/zotero/MildenhallEtAl2020/MildenhallEtAl2020-6-x35-y70.png]]</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p>[!Image]</p>
</blockquote>
<blockquote>
<p><!-- raw HTML omitted --> Image<!-- raw HTML omitted --></p>
</blockquote>
<blockquote>
<p>![[Archive/zotero/MildenhallEtAl2020/MildenhallEtAl2020-9-x158-y91.png]]</p>
</blockquote>
<blockquote>
</blockquote>
<hr>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://ownogatari.xyz/posts/r-fcn/">
    <span class="title">« Prev</span>
    <br>
    <span>R-FCN</span>
  </a>
  <a class="next" href="https://ownogatari.xyz/posts/516/">
    <span class="title">Next »</span>
    <br>
    <span>[paper review] Won Jun Oh (2023)</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    <link rel="stylesheet" href="./zotero.css">



<footer class="footer">
    
</footer>
</div>

</body>


</html></body>

</html>
