<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>미래사회와 창의혁신인재</title>
    <link>https://ownogatari.xyz/</link>
    <description>Recent content on 미래사회와 창의혁신인재</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 09 May 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://ownogatari.xyz/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Wonjun Oh</title>
      <link>https://ownogatari.xyz/posts/wonjunoh/</link>
      <pubDate>Thu, 09 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://ownogatari.xyz/posts/wonjunoh/</guid>
      <description>Introduction I am a junior student in SKKU. I am interested in Deeplearning and AI.
Skills language python c &amp;amp; cpp frameworks pytorch Education degerees: Bachelor schools: skku majors: dkl and swe Achievements and Certifications TOEIC : 860 Contact email: ownvoy@g.skku.edu github: https://github.com/ownvoy </description>
    </item>
    
    <item>
      <title>Swin Transformer</title>
      <link>https://ownogatari.xyz/posts/swin/</link>
      <pubDate>Mon, 29 Jan 2024 23:20:05 +0900</pubDate>
      
      <guid>https://ownogatari.xyz/posts/swin/</guid>
      <description>1. overview 1.1 objective Transformer에 inductive bias를 주입하자. Transformer의 계산 효율성을 높여보자. 1.2 background 1.2.1 ViT 기존 ViT의 문제점으로 2가지가 있다.
Fixed scale 기본적으로 16크기의 Patch를 쓴다. 16*16을 한 뭉터기로 보는 것은 inductive bias를 갖기에 너무 크다. CV같은 다양한 scale로 보는 것이 중요하다.(ex: FPN) complexity Image Segmentation 이나, Depth Estimation 같은 경우, pixel 별로 prediction이 일어나기에, 높은 해상도를 필요로 한다. ViT의 attention 경우 \(O(N^2)\)의 시간복잡도를 가진다. Swin Transformer의 경우 Patch Merging을 통해 (1)의 문제를 Window Attention을 통해 (2)의 문제를 해결한다.</description>
    </item>
    
    <item>
      <title>DETR</title>
      <link>https://ownogatari.xyz/posts/detr/</link>
      <pubDate>Fri, 19 Jan 2024 13:28:40 +0900</pubDate>
      
      <guid>https://ownogatari.xyz/posts/detr/</guid>
      <description>1. overview 1.1 objective detection에서 transformer를 사용해보자. detection에서 end to end 학습을 해보자. NMS, anchor setting의 부차적인 과정들이 학습의 결과에 많은 영향을 미친다. Transformer를 통해 위 두 개의 과정을 없앨 수 있게 된다.
2. main 2.1 Transformer DETR은 기본적으로 Transforemer 구조를 따른다. DeTR에서 Encoder와 Decoder가 무슨 역할을 하는지 보도록 한다.
2.1.1 Encoder cnn을 통과한 feature-map이 쪼개져서 input으로 들어온다. (ViT 스타일)
그 후, attention 계산을 통해 그림 전체에 대한 정보를 본다.</description>
    </item>
    
    <item>
      <title>TextCNN</title>
      <link>https://ownogatari.xyz/posts/textcnn/</link>
      <pubDate>Wed, 17 Jan 2024 20:52:14 +0900</pubDate>
      
      <guid>https://ownogatari.xyz/posts/textcnn/</guid>
      <description>1. overview 1.1 objective pretrained된 word2vec을 여러가지 task에 적용해볼까? 2. main 2.1 architecture word2vec + cnn
2.1.1 word2vec 아래와 같이 사전학습된 word2vec을 사용할 것이다. 이 word embedding은 학습을 하면서 고정될 수도 있고, 바뀔 수도 있는데 각각을 CNN-static, CNN-non-static이라고한다.
또 CNN-static과 CNN-non-static 두개 다 사용할 수 있는데, 이 경우 multi channel임으로, CNN-multichannel이라고한다. \(\Rightarrow\) overfitting 방지 (resnet 느낌)
2.1.2 CNN CNN은 filter를 학습한다. 근데 filter의 크기가 \(h\times k\)로 다소 rough한 형태이다.
또, convolutional layer 1개랑 pooling 1번, FCN 1번의 간단한 형태.</description>
    </item>
    
    <item>
      <title>FastText</title>
      <link>https://ownogatari.xyz/posts/fasttext/</link>
      <pubDate>Fri, 12 Jan 2024 15:48:20 +0900</pubDate>
      
      <guid>https://ownogatari.xyz/posts/fasttext/</guid>
      <description>1. overview 1.1 objective 단어를 형태소의 측면에서 표현해보자.
1.2 background 1.2.1 형태소적 접근 기존의 단어 표현은 하나의 단어당 하나의 벡터로 이루어졌다. 하나의 단어를 여러 형태소의 벡터로 표현해보자. 먹음에 대해서 먹기, 먹이, 먹는, 먹다, 먹었음, 먹었다 등 여러 형태가 나타날 수 있다. 그러나, 훈련 데이터에는 모든 형태가 등장하지 않는다. 그렇게 되면, 등장하지 않는 단어에 대해서representation이 떨어질 수 있다. 만약 형태소로 접근한다면, -기, -이, -는의 형태는 다른 단어들에서 볼 수 있다. 즉, 먹었음이 훈련데이터에 없다해도, 먹-, -었-, -음의 합으로 표현할 수 있다.</description>
    </item>
    
    <item>
      <title>RetinaNet</title>
      <link>https://ownogatari.xyz/posts/retinanet/</link>
      <pubDate>Wed, 03 Jan 2024 05:27:37 +0900</pubDate>
      
      <guid>https://ownogatari.xyz/posts/retinanet/</guid>
      <description>1. overview 1.1 objective 배경과 사물의 imbalance를 loss를 새롭게 만듦으로써 해결해보자.
1.2 background 1.2.1 Class Imbalance object는 몇 개 안 되는데, 배경은 \(10^4\sim10^5\)개 정도 있다. 배경을 많이 학습하는 것은 모델 성능 향상에 큰 도움이 안 된다.
1.2.2 Robust Loss Robust Loss: outlier(hard examples)를 down weight 해주는 loss Focal Loss: inliner(easy examples)를 down weight해주는 loss
1.2.3 Cross Entropy Loss Binary Cross Entropy Loss $$ - (y\log(p)+ (1-y)\log(1-p)), \ where \ y\in \lbrace 0,1 \rbrace$$</description>
    </item>
    
    <item>
      <title>CornerNet</title>
      <link>https://ownogatari.xyz/posts/cornernet/</link>
      <pubDate>Tue, 02 Jan 2024 15:06:43 +0900</pubDate>
      
      <guid>https://ownogatari.xyz/posts/cornernet/</guid>
      <description>1. overview 1.1 objective anchor box를 없애보자. 1.2 background 1.2.1 anchor box의 문제점 너무 많은 수의 anchor box (ex. DSS에는 40k 이상, RetinaNet에는 100k 이상) \(\Rightarrow\) positive sample과 negative sample의 imbalance가 training 속도 느리게 함. hyperparameter \(\uparrow\) (ex. ratio, box size, feature map size) 2. main 2.1 architecture backbone(Hourglass Networ) + prediction Module(Corner pooling, Heatmaps, Embeddings, offsets)으로 이루어져 있다.
2.1.1 Corner pooling 각각의 코너를 top-left corner와 bottom-right corner라 한다.
주황색 코너를 봐보면, 사람에 대한 local 정보가 없다.</description>
    </item>
    
    <item>
      <title>FPN</title>
      <link>https://ownogatari.xyz/posts/fpn/</link>
      <pubDate>Tue, 26 Dec 2023 06:25:59 +0900</pubDate>
      
      <guid>https://ownogatari.xyz/posts/fpn/</guid>
      <description>1. overview 1.1 objective resolution과 semantic의 trade-off를 줄여보자.
\(\Leftrightarrow\) 모든 resolution(scale)에서 강한 semantic 정보를 얻어보자.
1.2 background 1.2.1 resolution semantic trade-off \(\text{resolution} \downarrow \ \ \Rightarrow \ \ \text{semantic} \uparrow (😀) , \ \ \text{detail} \downarrow (🙁)\) \(\text{resolution} \uparrow \ \ \Rightarrow \ \ \text{semantic} \downarrow (🙁) \ , \text{detail} \uparrow (😀)\)
아래 table의 각 col들은 같은 의미임.
higher resolution lower resolution weak semantic strong semantic low level feature high level feature detail abstract lower layer higher layer 1.</description>
    </item>
    
    <item>
      <title>Mask R-CNN</title>
      <link>https://ownogatari.xyz/posts/mask_r-cnn/</link>
      <pubDate>Sat, 23 Dec 2023 13:42:52 +0900</pubDate>
      
      <guid>https://ownogatari.xyz/posts/mask_r-cnn/</guid>
      <description>1. overview 1.1 objective Faster R-CNN 돌리는김에 segmentation 해보자. 1.2 background detection sementic segmentation instance segementation 대상 각각이 어떤 물체인지 각각이 어떤 클래스이고, instance를 구별x 각각이 어떤 클래스이고, instance를 구별 범위 bounding box마다 전체의 pixel마다 bounding box의 pixel마다 대표 모델 Faster R-CNN FCN Mask R-CNN , FCIS 예시 Fater R-CNN: sementic segmentation 못함. (RoIPooling이 한 뭉태기로 하니까, pixel에 대한 각각의 정보 소멸) FCN: instance segmentation 못함. (전체 픽셀별로 하니까) 2. main mask R-CNN은 RoI마다 mask(K class에 대해)를 내놓기에, instance segmentation 가능함.</description>
    </item>
    
    <item>
      <title>SSD</title>
      <link>https://ownogatari.xyz/posts/ssd/</link>
      <pubDate>Tue, 19 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ownogatari.xyz/posts/ssd/</guid>
      <description>1. overview 1.1 objective multiple scale에 대해 다뤄 보자. (큰 물체, 작은 물체) \(\Rightarrow\) 장점: low resolution에 대한 문제를 해결 \(\Rightarrow\) 방법: 여러 개의 conv layer 도입
different shape에 대해 다뤄 보자. (박스 형태, 2:1, 1:1) \(\Rightarrow\) 장점: 다양한 형태의 객체 검출 \(\Rightarrow\) 방법: 박스 형태 다양하게
1.2 background YOLO YOLO 같은 경우 feature map이 \(7\times7\)짜리 1개이다. box개수는 각 cell마다 2개 있어서 총 98개이다.
2. main 2.1 architecture pretrained model + convolutional layer 논문에서는 VGG를 pretrained model로 사용</description>
    </item>
    
    <item>
      <title>R-FCN</title>
      <link>https://ownogatari.xyz/posts/r-fcn/</link>
      <pubDate>Thu, 23 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ownogatari.xyz/posts/r-fcn/</guid>
      <description>1. overview 1.1 objective translation invariance 문제를 해결해보자. 모델을 fully convolutional 하게 만들어보자. 1.2 Background translational invariance vs translational variance translational invariance의 정의 positional invariance(translation invariance): 위치가 변하여도 결과가 똑같아야함 = 위치가 영향을 주지 않음 image classification에서의 주요 과제
cnn은 translational invaraince하다.
weight sharing convolutiona filter를 활용한 계산은 원래 translational equivariance(translational variance)함. 층이 깊어질 수록 tralational invariance가 됨. 그 이유는 계속 같은 필터를 써서(weight sharing) max pooling max pooling 역시 translational invariance한 연산 cnn은 어떤 위치에 사물이 있어도 잘 classify한다.</description>
    </item>
    
    <item>
      <title>R-CNN</title>
      <link>https://ownogatari.xyz/posts/r-cnn/</link>
      <pubDate>Sun, 12 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ownogatari.xyz/posts/r-cnn/</guid>
      <description>1. overview 1.1 objective CNN을 object detection에 적용해보자.
1.2 background 1.2.1 object detection object detection은 사물이 뭔지, 어디에 있는지 찾는 것. classification과 localization을 동시에 하는 것. 1.2.2 way to localize object 가장 greedy한 방법: sliding window 이미지 크기: \(H\times W\) 박스 크기: \(h \times w\) 가능한 \(x\) 위치: \(W-w+1\) 가능한 \(y\) 위치: \(H-h +1\) 가능한 박스 위치: \((W-w+1)(H-h+1)\) 가능한 총 경우의 수 $$\sum_{h=1}^{H}\sum_{w=1}^{W}(W-w+1)(H-h+1)$$ is equal to $$\frac{H(H+1)}{2}\frac{W(W+1)}{2}$$ 만약 이미지의 크기가 300*300이라고 하면 81,000,000의 경우의 수가 나옴.</description>
    </item>
    
    <item>
      <title>Ddpg</title>
      <link>https://ownogatari.xyz/posts/ddpg/</link>
      <pubDate>Sat, 23 Sep 2023 00:53:01 +0900</pubDate>
      
      <guid>https://ownogatari.xyz/posts/ddpg/</guid>
      <description>0. BackGround observation: \(x_t\)
state: \(s_t\)
state: \(a_t\)
reward: \(r_t\)
policy: \(\pi, \ \ S \to P(A)\)
transition dynamics : \(p(s_{t+1} \mid s_t,a_t)\)
discounted future reward : \(R_T = \sum_{i=t}^{T}\gamma^{(i-t)}r(s_i,a_i)\)
objective function: \(E_{r_i,s_i \sim E, a_i \sim \pi}[R_1]\)
\(Q^{\pi}(s_t,a_t) = E_{r_i\geq t, s_i &amp;gt;t \sim E , a_i &amp;gt; t \sim \pi}[R_t \mid s_t, a_t]\)
\(Q^{\pi}(s_t,a_t) = E_{r_t,s_{t+1} \sim E} [r(s_t,a_t)+ \gamma E_{a_{t+1} \sim \pi} [Q^{\pi}(s_{t+1}, a_{t+1})]]\)
\(Q^{\mu}(s_t,a_t) = E_{r_t,s_{t+1} \sim E} [r(s_t,a_t)+ \gamma Q^{\mu}(s_{t+1}, \mu(s_{t+1}))]\)</description>
    </item>
    
    <item>
      <title>Test</title>
      <link>https://ownogatari.xyz/docs/test/</link>
      <pubDate>Fri, 15 Sep 2023 23:53:47 +0900</pubDate>
      
      <guid>https://ownogatari.xyz/docs/test/</guid>
      <description>This is a test page!! I am going to write dev blogs.</description>
    </item>
    
  </channel>
</rss>
