<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>NLP on 미래사회와 창의혁신인재</title>
    <link>https://ownogatari.xyz/categories/nlp/</link>
    <description>Recent content in NLP on 미래사회와 창의혁신인재</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Jan 2024 20:52:14 +0900</lastBuildDate><atom:link href="https://ownogatari.xyz/categories/nlp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>TextCNN</title>
      <link>https://ownogatari.xyz/posts/textcnn/</link>
      <pubDate>Wed, 17 Jan 2024 20:52:14 +0900</pubDate>
      
      <guid>https://ownogatari.xyz/posts/textcnn/</guid>
      <description>1. overview 1.1 objective pretrained된 word2vec을 여러가지 task에 적용해볼까? 2. main 2.1 architecture word2vec + cnn
2.1.1 word2vec 아래와 같이 사전학습된 word2vec을 사용할 것이다. 이 word embedding은 학습을 하면서 고정될 수도 있고, 바뀔 수도 있는데 각각을 CNN-static, CNN-non-static이라고한다.
또 CNN-static과 CNN-non-static 두개 다 사용할 수 있는데, 이 경우 multi channel임으로, CNN-multichannel이라고한다. \(\Rightarrow\) overfitting 방지 (resnet 느낌)
2.1.2 CNN CNN은 filter를 학습한다. 근데 filter의 크기가 \(h\times k\)로 다소 rough한 형태이다.
또, convolutional layer 1개랑 pooling 1번, FCN 1번의 간단한 형태.</description>
    </item>
    
    <item>
      <title>FastText</title>
      <link>https://ownogatari.xyz/posts/fasttext/</link>
      <pubDate>Fri, 12 Jan 2024 15:48:20 +0900</pubDate>
      
      <guid>https://ownogatari.xyz/posts/fasttext/</guid>
      <description>1. overview 1.1 objective 단어를 형태소의 측면에서 표현해보자.
1.2 background 1.2.1 형태소적 접근 기존의 단어 표현은 하나의 단어당 하나의 벡터로 이루어졌다. 하나의 단어를 여러 형태소의 벡터로 표현해보자. 먹음에 대해서 먹기, 먹이, 먹는, 먹다, 먹었음, 먹었다 등 여러 형태가 나타날 수 있다. 그러나, 훈련 데이터에는 모든 형태가 등장하지 않는다. 그렇게 되면, 등장하지 않는 단어에 대해서representation이 떨어질 수 있다. 만약 형태소로 접근한다면, -기, -이, -는의 형태는 다른 단어들에서 볼 수 있다. 즉, 먹었음이 훈련데이터에 없다해도, 먹-, -었-, -음의 합으로 표현할 수 있다.</description>
    </item>
    
  </channel>
</rss>
